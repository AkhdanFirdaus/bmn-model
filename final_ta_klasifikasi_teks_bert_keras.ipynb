{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DnbLk8zFapDr",
        "fKXBbev7iLai",
        "i6T77ib8PdHz",
        "S4C8djJfPieR",
        "q8gj5Hk2Plxz",
        "fUchNe_lPgwY",
        "VM5ADuHIa1vN",
        "4fvFP7eaO2V2",
        "ivTrXNzOPAGz",
        "3w8LsS7Rtq6a"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOvmpmZD9PEqHqGLrtZBzRh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkhdanFirdaus/bmn-model/blob/main/final_ta_klasifikasi_teks_bert_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Klasifikasi Teks Laporan Mobil Dinas dengan BERT"
      ],
      "metadata": {
        "id": "p17CKw6O7IEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "o2iONI4WK28C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "fKXBbev7iLai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e81-YvWKHEAX",
        "outputId": "202e30ce-696c-4553-aa63-d240bddcd021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loc_master = '/content/drive/MyDrive/Kuliah/TA Akhdan Musyaffa Firdaus/Bimbingan - Akhdan Musyaffa Firdaus/Dataset/Raw Data/raw_master_kendaraan.csv'\n",
        "loc_invoice = '/content/drive/MyDrive/Kuliah/TA Akhdan Musyaffa Firdaus/Bimbingan - Akhdan Musyaffa Firdaus/Dataset/Raw Data/raw_invoice.csv'\n",
        "loc_sukucadang = '/content/drive/MyDrive/Kuliah/TA Akhdan Musyaffa Firdaus/Bimbingan - Akhdan Musyaffa Firdaus/Dataset/Raw Data/raw_sukucadang.csv'\n",
        "loc_kerusakan = '/content/drive/MyDrive/Kuliah/TA Akhdan Musyaffa Firdaus/Bimbingan - Akhdan Musyaffa Firdaus/Dataset/Raw Data/raw_kategori_kerusakan.csv'\n",
        "loc_laporan = '/content/drive/MyDrive/Kuliah/TA Akhdan Musyaffa Firdaus/Bimbingan - Akhdan Musyaffa Firdaus/Dataset/Raw Data/raw_laporan.csv'\n",
        "\n",
        "data_master_raw = pd.read_csv(loc_master)\n",
        "data_invoice_raw = pd.read_csv(loc_invoice)\n",
        "data_sukucadang_raw = pd.read_csv(loc_sukucadang)\n",
        "data_kerusakan_raw = pd.read_csv(loc_kerusakan)\n",
        "data_laporan_raw = pd.read_csv(loc_laporan)"
      ],
      "metadata": {
        "id": "WCH7ZoHInVdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 Data Preparation"
      ],
      "metadata": {
        "id": "k3kcXp8wZPbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3.1 Pemilahan Data"
      ],
      "metadata": {
        "id": "DnbLk8zFapDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perbaikan Data"
      ],
      "metadata": {
        "id": "S4C8djJfPieR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZueYa_g8me1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3.2 Pembersihan Data"
      ],
      "metadata": {
        "id": "VM5ADuHIa1vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spell Checker"
      ],
      "metadata": {
        "id": "4fvFP7eaO2V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spell Checker"
      ],
      "metadata": {
        "id": "uXejUPny_CDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Casefolding"
      ],
      "metadata": {
        "id": "XQdtMgWcOu9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def casefolding(val):\n",
        "  return str(val).lower()"
      ],
      "metadata": {
        "id": "uC_3iCTYlv6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning"
      ],
      "metadata": {
        "id": "3w8LsS7Rtq6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clearformat(val):\n",
        "  # Membersihkan Whitespace\n",
        "  val = re.sub(r'\\s+', ' ', val)\n",
        "\n",
        "  # Hanya Mengambil karakter alfanumerik\n",
        "  val = re.sub(\"[^a-zA-Z0-9]\", \" \", val)\n",
        "\n",
        "  return val"
      ],
      "metadata": {
        "id": "mxK0OMdqtp53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "OCK7voEvO8Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hanya dilakuan pada\n",
        "- data sukucadang = Keterangan Pekerjaan\n",
        "- data kerusakan = Keterangan Kerusakan\n",
        "- data level = keterangan level\n",
        "\n"
      ],
      "metadata": {
        "id": "oN9v6w_DHxud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Sastrawi PySastrawi"
      ],
      "metadata": {
        "id": "kPnEFyJucycn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "def stemming(val):\n",
        "  return stemmer.stem(str(val))"
      ],
      "metadata": {
        "id": "Ha_x_6GQ_GrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopword Removal"
      ],
      "metadata": {
        "id": "ivTrXNzOPAGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hanya dilakuan pada\n",
        "- data sukucadang = Keterangan Pekerjaan\n",
        "- data kerusakan = Keterangan Kerusakan\n",
        "- data level = keterangan level"
      ],
      "metadata": {
        "id": "kP2gFn1uIJFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "stopword_remover = StopWordRemoverFactory().create_stop_word_remover()\n",
        "\n",
        "def stopwordremove(val):\n",
        "  return stopword_remover.remove(str(val))"
      ],
      "metadata": {
        "id": "r41OvxAwfNqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Data"
      ],
      "metadata": {
        "id": "vNj30WPI3paV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4phC5dGFmJx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3.3 Konstruksi Data"
      ],
      "metadata": {
        "id": "f0dhHfUda7ZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penentuan Fitur"
      ],
      "metadata": {
        "id": "FZpaJMOX3ZYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Akan dibuat sebuah dataset untuk mendeteksi konteks\n",
        "1. menggabungkan data master dan invoice berdasar nomor polisi\n",
        "2. menggabungkan data diatas dengan sukucadang berdasar pekerjaan"
      ],
      "metadata": {
        "id": "8Cb22KnnIoMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pemilihan Atribut"
      ],
      "metadata": {
        "id": "uLBKm1v1k72N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3SNV4exk_Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3.4 Pelabelan Data"
      ],
      "metadata": {
        "id": "11HpoRWsa_P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penentuan Label"
      ],
      "metadata": {
        "id": "9OJtf64G4AUN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcek_Vp1lT-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penerapan Label"
      ],
      "metadata": {
        "id": "IsfQz2tW4pmb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ly4cXLHilTXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3.5 Integrasi Data"
      ],
      "metadata": {
        "id": "oqbLHnGObDDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penggabungan Data"
      ],
      "metadata": {
        "id": "CXIresmr4QI9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "036JNAq2QGvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pembentukan Dataset Akhir"
      ],
      "metadata": {
        "id": "-4OL4KTP4gZT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZ2rpTEYldMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balancing Data"
      ],
      "metadata": {
        "id": "ST5hwz6qlhRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Dataset"
      ],
      "metadata": {
        "id": "4SBCQuud4UNz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ov5U39z1lqms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.4 Modeling"
      ],
      "metadata": {
        "id": "b46qsYWcZbch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/Kuliah/TA Akhdan Musyaffa Firdaus/Bimbingan - Akhdan Musyaffa Firdaus/Dataset/Dipilah/final_dataset3.csv', sep='\\t')\n",
        "LABELS = dataset.columns[1:]"
      ],
      "metadata": {
        "id": "h1Zf31EUgoKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers tensorflow Sastrawi"
      ],
      "metadata": {
        "id": "xkHsolrl7EBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# tf.config.run_functions_eagerly(True)\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rgmQPgWnrMpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare model variable and hyperparameters\n",
        "MAX_LEN=256\n",
        "TRAIN_BATCH_SIZE=32\n",
        "TEST_BATCH_SIZE=32\n",
        "EPOCHS=10\n",
        "LEARNING_RATE=2e-05\n",
        "TRAIN_SPLIT=0.8\n",
        "TEST_SPLIT=0.2"
      ],
      "metadata": {
        "id": "5rEOr_LCRoNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Pre-trained Model dan Tokenizer\n",
        "pretrainedbertdataset = 'indobenchmark/indobert-lite-base-p1'\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(pretrainedbertdataset)\n",
        "bert_model = TFBertModel.from_pretrained(pretrainedbertdataset)"
      ],
      "metadata": {
        "id": "duL5IP0oKd95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd502804-eb17-4a2d-91ec-9473f0c5a277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'AlbertTokenizerFast'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "You are using a model of type albert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFBertModel: ['predictions', 'albert', 'sop_classifier']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertModel were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['bert']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing Function\n",
        "def tokenize_data(tokenizer, max_len, val):\n",
        "  tokenized = tokenizer.encode_plus(\n",
        "      val,\n",
        "      add_special_tokens=True,\n",
        "      max_length=max_len,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=True,\n",
        "      return_tensors='tf'\n",
        "  )\n",
        "\n",
        "  return tokenized\n",
        "\n",
        "def tokenize_sentences(tokenizer, max_len, sentences):\n",
        "  input_ids, attention_mask = [], []\n",
        "  for sentence in sentences:\n",
        "    tokenized = tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=False,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    input_ids.append(tokenized['input_ids'])\n",
        "    attention_mask.append(tokenized['attention_mask'])\n",
        "\n",
        "  return {\n",
        "    'input_ids': tf.convert_to_tensor(np.asarray(input_ids).squeeze(), dtype=tf.int32),\n",
        "    'attention_mask': tf.convert_to_tensor(np.asarray(attention_mask).squeeze(), dtype=tf.int32)\n",
        "  }"
      ],
      "metadata": {
        "id": "l1ggbkYNLQzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "stopword_remover = StopWordRemoverFactory().create_stop_word_remover()\n",
        "\n",
        "def casefolding(val):\n",
        "  return str(val).lower()\n",
        "\n",
        "def stemming(val):\n",
        "  return stemmer.stem(str(val))\n",
        "\n",
        "def stopwordremove(val):\n",
        "  return stopword_remover.remove(str(val))\n",
        "\n",
        "def preprocess_sentences(tokenizer, max_len, sentences):\n",
        "  input_ids, attention_mask = [], []\n",
        "  for sentence in sentences:\n",
        "    input = casefolding(sentence)\n",
        "    input = stemming(input)\n",
        "    input = stopwordremove(input)\n",
        "    tokenized = tokenizer.encode_plus(\n",
        "        input,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=False,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    input_ids.append(tokenized['input_ids'])\n",
        "    attention_mask.append(tokenized['attention_mask'])\n",
        "\n",
        "  return {\n",
        "    'input_ids': tf.convert_to_tensor(np.asarray(input_ids).squeeze(), dtype=tf.int32),\n",
        "    'attention_mask': tf.convert_to_tensor(np.asarray(attention_mask).squeeze(), dtype=tf.int32)\n",
        "  }"
      ],
      "metadata": {
        "id": "Uu3OvY38_6ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percobaan penggunaan tokenisasi\n",
        "tokenized = tokenize_sentences(bert_tokenizer, MAX_LEN, [\n",
        "    'Mobil saya tidak bisa di starter',\n",
        "    'AC tidak dingin'\n",
        "])\n",
        "\n",
        "print(tokenized)"
      ],
      "metadata": {
        "id": "dWc3kD0z2Nlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataset['konteks'], dataset[LABELS], test_size=TEST_SPLIT, shuffle=True, random_state=42)\n",
        "X_train = tokenize_sentences(bert_tokenizer, MAX_LEN, X_train)\n",
        "X_test = tokenize_sentences(bert_tokenizer, MAX_LEN, X_test)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(TRAIN_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(TEST_BATCH_SIZE)"
      ],
      "metadata": {
        "id": "2rCYtGjgkJ2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_model(bert_encoder, num_labels, max_len, learning_rate):\n",
        "  input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
        "  attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "  encoding_layer = bert_encoder(input_ids, attention_mask)[0]\n",
        "\n",
        "  l = tf.keras.layers.GlobalAveragePooling1D(name='pooling_layer')(encoding_layer)\n",
        "  l = tf.keras.layers.Dropout(0.1, name='dropout_layer')(l)\n",
        "  l = tf.keras.layers.Dense(num_labels, activation='sigmoid', name='output_layer')(l)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[l])\n",
        "\n",
        "  OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  LOSS = tf.keras.losses.BinaryCrossentropy()\n",
        "  METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
        "\n",
        "  model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
        "\n",
        "  return model\n",
        "\n",
        "model = classification_model(\n",
        "    bert_encoder=bert_model,\n",
        "    num_labels=len(LABELS),\n",
        "    max_len=MAX_LEN,\n",
        "    learning_rate=LEARNING_RATE,\n",
        ")"
      ],
      "metadata": {
        "id": "ToFvhxqP9lH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AORNnOst93o4",
        "outputId": "dd84b5d9-e838-4fcd-d3cd-ec61b4eb8d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109081344   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 256,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " pooling_layer (GlobalAveragePo  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
            " oling1D)                                                                                         \n",
            "                                                                                                  \n",
            " dropout_layer (Dropout)        (None, 768)          0           ['pooling_layer[0][0]']          \n",
            "                                                                                                  \n",
            " output_layer (Dense)           (None, 11)           8459        ['dropout_layer[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,089,803\n",
            "Trainable params: 109,089,803\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "UfgwQ43fIn0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a190e9-1bfc-495b-b4e4-c7d67ece7f84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 74s 2s/step - loss: 0.5048 - accuracy: 0.8147 - val_loss: 0.3267 - val_accuracy: 0.8968\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.3396 - accuracy: 0.8814 - val_loss: 0.2826 - val_accuracy: 0.8906\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.2812 - accuracy: 0.8892 - val_loss: 0.2567 - val_accuracy: 0.9045\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 12s 2s/step - loss: 0.2294 - accuracy: 0.9032 - val_loss: 0.2111 - val_accuracy: 0.9384\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.1862 - accuracy: 0.9263 - val_loss: 0.1935 - val_accuracy: 0.9353\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.1512 - accuracy: 0.9504 - val_loss: 0.1617 - val_accuracy: 0.9553\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.1283 - accuracy: 0.9493 - val_loss: 0.1368 - val_accuracy: 0.9538\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.1035 - accuracy: 0.9645 - val_loss: 0.1246 - val_accuracy: 0.9661\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.0864 - accuracy: 0.9692 - val_loss: 0.1139 - val_accuracy: 0.9676\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 13s 2s/step - loss: 0.0717 - accuracy: 0.9750 - val_loss: 0.1187 - val_accuracy: 0.9646\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe6c60d3700>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.5 Model Evaluation"
      ],
      "metadata": {
        "id": "zFff9onQZj2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "HAbCVcEqSQqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b087953-5364-422c-9881-32b8763886f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 504ms/step - loss: 0.1187 - accuracy: 0.9646\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11871889233589172, 0.964560866355896]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = ['AC mobil tidak dingin', 'Rem pada mobil tidak bekerja', 'Kaca spion rusak harus diganti']\n",
        "tokenized = preprocess_sentences(bert_tokenizer, MAX_LEN, inputs)\n",
        "predictions = model.predict(tokenized)\n",
        "\n",
        "# Define the threshold\n",
        "threshold = 0.5\n",
        "\n",
        "# Apply the threshold to the predictions\n",
        "binary_predictions = np.where(predictions > threshold, 1, 0)\n",
        "\n",
        "# Print the binary predictions\n",
        "print(LABELS)\n",
        "print(binary_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hOOj9e3sqhd",
        "outputId": "8372126c-0030-42ab-e75c-1985de5150ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 7s 7s/step\n",
            "Index(['masalah sistem knalpot', 'masalah transmisi', 'masalah suspensi',\n",
            "       'gangguan listrik', 'masalah sistem bahan bakar', 'masalah kemudi',\n",
            "       'masalah ban', 'masalah mesin', 'kegagalan rem',\n",
            "       'masalah sistem pendingin', 'kerusakan aksesoris interior eksterior'],\n",
            "      dtype='object')\n",
            "[[0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming you have predictions and ground truth labels\n",
        "predictions = model.predict(test_dataset)  # Get model predictions\n",
        "threshold = 0.5  # Set threshold for label prediction\n",
        "\n",
        "# Convert probability predictions to binary predictions\n",
        "binary_predictions = np.where(predictions >= threshold, 1, 0)\n",
        "\n",
        "# Compute precision, recall, and F1-score for each label\n",
        "label_precisions = precision_score(y_test, binary_predictions, average=None)\n",
        "label_recalls = recall_score(y_test, binary_predictions, average=None)\n",
        "label_f1_scores = f1_score(y_test, binary_predictions, average=None)\n",
        "\n",
        "# Print accuracy metrics for each label\n",
        "for label, precision, recall, f1_score in zip(LABELS, label_precisions, label_recalls, label_f1_scores):\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1-score: {f1_score}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKWbDm0E33Tq",
        "outputId": "6a0c0904-f553-4135-ad19-a317b3ed9969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 5s 468ms/step\n",
            "Label: masalah sistem knalpot\n",
            "Precision: 1.0\n",
            "Recall: 0.8333333333333334\n",
            "F1-score: 0.9090909090909091\n",
            "\n",
            "Label: masalah transmisi\n",
            "Precision: 0.8888888888888888\n",
            "Recall: 1.0\n",
            "F1-score: 0.9411764705882353\n",
            "\n",
            "Label: masalah suspensi\n",
            "Precision: 0.3333333333333333\n",
            "Recall: 0.3333333333333333\n",
            "F1-score: 0.3333333333333333\n",
            "\n",
            "Label: gangguan listrik\n",
            "Precision: 0.875\n",
            "Recall: 1.0\n",
            "F1-score: 0.9333333333333333\n",
            "\n",
            "Label: masalah sistem bahan bakar\n",
            "Precision: 1.0\n",
            "Recall: 0.6666666666666666\n",
            "F1-score: 0.8\n",
            "\n",
            "Label: masalah kemudi\n",
            "Precision: 0.7142857142857143\n",
            "Recall: 1.0\n",
            "F1-score: 0.8333333333333333\n",
            "\n",
            "Label: masalah ban\n",
            "Precision: 0.75\n",
            "Recall: 0.8571428571428571\n",
            "F1-score: 0.7999999999999999\n",
            "\n",
            "Label: masalah mesin\n",
            "Precision: 1.0\n",
            "Recall: 0.3333333333333333\n",
            "F1-score: 0.5\n",
            "\n",
            "Label: kegagalan rem\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "\n",
            "Label: masalah sistem pendingin\n",
            "Precision: 0.8\n",
            "Recall: 1.0\n",
            "F1-score: 0.888888888888889\n",
            "\n",
            "Label: kerusakan aksesoris interior eksterior\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.6 Deployment"
      ],
      "metadata": {
        "id": "IYYwqoxdZmna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm model_output/klasifikasi.h5\n",
        "model.save('model_output/klasifikasi.h5')"
      ],
      "metadata": {
        "id": "iMgrrgLH1Dxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03873de2-fc07-44b7-c1ca-072ea15eee99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'model_output/klasifikasi2.h5': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model('/content/model_output/klasifikasi.h5', custom_objects={'TFBertModel': TFBertModel})"
      ],
      "metadata": {
        "id": "3KQu0FJFdUcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = ['mesin tidak bisa menyala', 'AC mobil tidak dingin', 'Mesin bersuara kasar', 'ban bocor dan harus diganti dengan yang baru', 'asap knalpot berwarna putih', 'kaca spion pecah dan harus diganti', 'lampu sein kendaraan tidak berfungsi']\n",
        "tokenized = preprocess_sentences(bert_tokenizer, MAX_LEN, inputs)\n",
        "predictions = loaded_model.predict(tokenized)\n",
        "\n",
        "# Define the threshold\n",
        "threshold = 0.5\n",
        "\n",
        "# Apply the threshold to the predictions\n",
        "binary_predictions = np.where(predictions > threshold, 1, 0)\n",
        "\n",
        "# Print the binary predictions\n",
        "print(LABELS)\n",
        "print(binary_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwG4B47PvKqj",
        "outputId": "8bc5a84a-ab9e-4d4d-e9f6-192a79886173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n",
            "Index(['masalah sistem knalpot', 'masalah transmisi', 'masalah suspensi',\n",
            "       'gangguan listrik', 'masalah sistem bahan bakar', 'masalah kemudi',\n",
            "       'masalah ban', 'masalah mesin', 'kegagalan rem',\n",
            "       'masalah sistem pendingin', 'kerusakan aksesoris interior eksterior'],\n",
            "      dtype='object')\n",
            "[[0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementasi Rumus menentukan tingkat kerusakan"
      ],
      "metadata": {
        "id": "ue0bMkisEGrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_WEIGHTS = dict.fromkeys(label_kerusakan, 0)\n",
        "\n",
        "def measure_severity(val):\n",
        "  treshold = 0.5\n",
        "\n",
        "  NUMBER_LABEL_DETECTED = np.sum(val > treshold)\n",
        "  LABEL_RESULTS = {}\n",
        "\n",
        "  for label, count in zip(LABEL_WEIGHTS.keys(), val):\n",
        "    if count > treshold:\n",
        "      LABEL_RESULTS[label] = count\n",
        "\n",
        "  category_weights = [LABEL_WEIGHTS[label] * count for label, count in LABEL_RESULTS.items()]\n",
        "  severity_score = sum(category_weights) / NUMBER_LABEL_DETECTED\n",
        "\n",
        "  return severity_score"
      ],
      "metadata": {
        "id": "tM6HaB9bEKhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}